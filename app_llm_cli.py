import os
import json
import logging
import re
from dotenv import load_dotenv
from pdfminer.high_level import extract_text
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_tavily import TavilySearch
from langchain_core.documents import Document
from rag_indexer_class import IndexConfig, RAGIndexer
from utils.index import image_to_base64


# pdfminer ê²½ê³  ë¬´ì‹œ
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")


def search_vector_db_image(img_path):
    """ë°±í„° ë””ë¹„ì—ì„œ ì´ë¯¸ì§€ì˜ ëª¨ë¸ì„ ê°€ì ¸ì˜¨ë‹¤"""

    # ì„¤ì • ìƒì„±
    config = IndexConfig(
        persistent_directory="./chroma",
        collection_name="imgs",
        embedding_model="text-embedding-3-small",
    )

    # ì¸ë±ì„œ ìƒì„± ë° ì‹¤í–‰
    indexer = RAGIndexer(config)

    # ì´ë¯¸ì§€ ë¡œë“œí•´ì„œ ëª¨ë¸ëª… ê²€ìƒ‰
    img_base64 = image_to_base64(img_path)

    # ìœ ì‚¬ë„ ê²€ìƒ‰
    model_nm = indexer.search_and_show(img_base64)
    return model_nm


def extract_text_from_pdf(pdf_path):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return extract_text(pdf_path)
    except Exception as e:
        print(f"PDF ì½ê¸° ì‹¤íŒ¨ {pdf_path}: {e}")
        return ""


def analyze_query_and_retrieve(query: str, retriever, llm):
    # ì§ˆë¬¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_messages(
        [        ('system', """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ ì§ˆë¬¸ì—ì„œ ë‹¤ìŒì„ ì¶”ì¶œí•˜ì„¸ìš”:
        
        1. ì£¼ìš” í‚¤ì›Œë“œ (3-5ê°œ)
        2. ì§ˆë¬¸ì˜ í•µì‹¬ ì£¼ì œ
        3. êµ¬ì²´ì ì¸ ì¡°ê±´ì´ë‚˜ ìš”êµ¬ì‚¬í•­
        4. ë‹µë³€ì—ì„œ ë‹¤ë¤„ì•¼ í•  ì„¸ë¶€ ì‚¬í•­ë“¤
        
        JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
        {{
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
            "main_topic": "ì£¼ì œ",
            "conditions": ["ì¡°ê±´1"],
            "details": ["ì„¸ë¶€ì‚¬í•­1"]
        }}""",
            ),
            ("human", "ì§ˆë¬¸: {query}"),
        ]
    )

    chain = prompt | llm | StrOutputParser()
    analysis_result = chain.invoke({"query": query})

    # JSON íŒŒì‹±
    try:
        data = json.loads(analysis_result)
        keywords = data.get("keywords", [])
    except Exception:
        keywords = [query]

    all_contexts = []

    # TavilySearchë¡œ ì›¹ ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
    tavily_tool = TavilySearch(max_results=5)

    try:
        # Tavilyì—ì„œ ê²€ìƒ‰
        search_result = tavily_tool.invoke({"query": query})

        # 'results' ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        web_results = search_result.get("results", [])

        # ê° ê²°ê³¼ë¥¼ Documentë¡œ ë³€í™˜
        for item in web_results:
            content = item.get("content", "")
            url = item.get("url", "")

            if content:  # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶”ê°€
                doc = Document(
                    page_content=content,
                    metadata={"source": url, "title": item.get("title", "")},
                )
                all_contexts.append(doc)
    except Exception as e:
        print(f"ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

    # ê¸°ì¡´ ë²¡í„° retrieverë„ ê²€ìƒ‰
    for keyword in keywords:
        try:
            docs = retriever.invoke(keyword)
            all_contexts.extend(docs)
        except Exception as e:
            print(f"ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            continue

    return all_contexts, analysis_result


def enhanced_chain(query: str, retriever, llm, cot_prompt, history=[]):
    context, analysis = analyze_query_and_retrieve(query, retriever, llm)
    prompt_value = cot_prompt.invoke(
        {"query": query, "analysis": analysis, "context": context}
    )

    prompt_str = prompt_value.to_string()

    # history + í˜„ì¬ ì§ˆë¬¸ promptë¥¼ í•©ì³ messages êµ¬ì„±
    messages = history + [{"role": "user", "content": prompt_str}]

    # LLMì— messages ì „ë‹¬
    response = llm.invoke(messages)

    return response


def run_chatbot(query, image_path=None, history=[]):
    EMBEDDINGS_MODEL = "text-embedding-3-small"
    COLLECTION_NAME = "manuals"
    VECTOR_DB_DIR = "./chroma"

    # ì„¤ì • ìƒì„±
    config = IndexConfig(
        persistent_directory=VECTOR_DB_DIR,
        collection_name=COLLECTION_NAME,
        embedding_model=EMBEDDINGS_MODEL,
    )

    # ì¸ë±ì„œ ìƒì„± ë° ì‹¤í–‰
    indexer = RAGIndexer(config)

    retriever = indexer.vectordb.as_retriever(
        search_type="mmr", search_kwargs={"k": 8, "fetch_k": 20}
    )

    if image_path:
        model_code = search_vector_db_image(image_path)
        if model_code == -1:
            query = f"{query} (ëª¨ë¸ì½”ë“œ: í™•ì¸ë¶ˆê°€)"
        else:
            query = f"{query} (ëª¨ë¸ì½”ë“œ: {model_code})"

    llm = ChatOpenAI(model=MODEL_NAME, temperature=0.3)

    cot_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """
            Elaborate on the topic using a Tree of Thoughts and backtrack when necessary to construct a clear, cohesive Chain of Thought reasoning.
            ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í•œ ê°€ì „ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¶„ì„í•œ í›„ì— ê´€ë ¨ ì •ë³´ë¥¼ ìˆ˜ì§‘í•œ í›„, ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.:
            ## ë‹µë³€ ì§€ì¹¨
            - ì¡°ê±´ë“¤ì„ ë‚˜ì—´í•˜ê¸°ë³´ë‹¤ëŠ” í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
            - ë°˜ë³µë˜ê±°ë‚˜ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì¤‘ë³µí•´ì„œ ì„¤ëª…í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
            - ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°–ì¶˜ ëª…í™•í•œ ë¬¸ë‹¨ í˜•íƒœë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
            - í•„ìš” ì‹œ ì˜ˆì‹œë‚˜ ìœ ì‚¬ ìƒí™©ì„ ë“¤ì–´ ì´í•´ë¥¼ ë„ìš°ì‹­ì‹œì˜¤.
            - í•­ëª©ë§ˆë‹¤ ê´€ë ¨ëœ ì´ëª¨ì§€ë¥¼ ë¶™ì´ê³ , ë³´ê¸° ì¢‹ê²Œ ì†Œì œëª©ì„ ì‚¬ìš©í•´ì„œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì£¼ì‹­ì‹œì˜¤.
            - ë§íˆ¬ëŠ” í•œêµ­ì–´ ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê³  ì˜ˆìœ ëŠë‚Œìœ¼ë¡œ í•´ì£¼ì‹­ì‹œì˜¤. 
            - ì¶”ê°€ë¡œ, ì£¼ì˜ì‚¬í•­ì€ ë”°ë¡œ 'ì¶”ê°€ ì•ˆë‚´' ì„¹ì…˜ìœ¼ë¡œ ë¹¼ì£¼ì„¸ìš”.
            
            ì˜ˆì‹œ ì¶œë ¥ :
            
       
            - [ì²´ê³„ì ì¸ í†µí•© ì„¤ëª…ì„ í•œ ë¬¸ë‹¨ ì´ìƒìœ¼ë¡œ ê¸°ìˆ ]

            ### ğŸ“Œ ì¶”ê°€ ì•ˆë‚´
            - [ê´€ë ¨ëœ íŒì´ë‚˜ ì°¸ê³  ì •ë³´ê°€ ìˆìœ¼ë©´ ì œê³µ]
            """,
                ),
                (
                    "human",
                    """
            ì§ˆë¬¸: {query}
            ë¶„ì„: {analysis}
            ì»¨í…ìŠ¤íŠ¸: {context}
            """,
                ),
            ]
        )


    result = enhanced_chain(query, retriever, llm, cot_prompt, history=history)
    return result.content


def main():
    print("=" * 60)
    print("ì„¸íƒê¸°/ê±´ì¡°ê¸° ë„ìš°ë¯¸")
    print("=" * 60)

    history = []

    while True:
        try:
            query = input(
                "\nâœ… ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì´ë¯¸ì§€ë¥¼ ì“¸ ë• img:/path/to/img, ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'): "
            ).strip()
            if query.lower() == "ì¢…ë£Œ":
                print("âœ…  ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not query:
                print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            # ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥ ì²˜ë¦¬
            if query.startswith("img:"):
                raw_path = query[4:].strip()

                # í™•ì¥ì ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œë¥¼ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ
                # ì˜ˆ: img:/home/user/test image.jpg something
                match = re.search(
                    r"(.+?\.(?:png|jpg|jpeg|webp))", raw_path, re.IGNORECASE
                )
                if not match:
                    print("ì˜¬ë°”ë¥¸ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤.")
                    continue

                img_path = match.group(1).strip()

                if not os.path.exists(img_path):
                    print("ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                print("âœ…  ì´ë¯¸ì§€ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ ì¤‘...")
                model_nm = search_vector_db_image(img_path)
                print(f"âœ… ê°ì§€ëœ ëª¨ë¸ëª…: {model_nm}")
                user_question = input("ğŸ’¬ ì§ˆë¬¸ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if not user_question:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                # ëª¨ë¸ëª…ì„ ì§ˆë¬¸ì— ì¶”ê°€
                query = f"{user_question} (ëª¨ë¸ëª…: {model_nm})"

            # historyì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
            history.append({"role": "user", "content": query})

            print("ğŸ” ë‹µë³€ ìƒì„± ì¤‘...")
            # run_chatbotì´ historyë¥¼ ì´ìš©í•´ ë¬¸ë§¥ ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ì„¤ê³„
            result = run_chatbot(query, history=history)

            # historyì— ëª¨ë¸ ì‘ë‹µ ì¶”ê°€
            history.append({"role": "assistant", "content": result.content})

            print("=" * 60)
            print(result.content)
            print("=" * 60)

        except KeyboardInterrupt:
            print("\nâœ… ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
