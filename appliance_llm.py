import os
import json
import logging
from dotenv import load_dotenv
from pdfminer.high_level import extract_text
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document

# âœ… Macì—ì„œ pdfminer ê²½ê³  ë¬´ì‹œ
logging.getLogger("pdfminer").setLevel(logging.ERROR)

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")

if not OPENAI_API_KEY:
    print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# âœ… íŒŒì¼ ê²½ë¡œ (Macì—ì„œë„ ì ˆëŒ€ê²½ë¡œë¡œ ì•ˆì „í•˜ê²Œ)
FILE_PATH = os.path.abspath("./data/samsung_manuals/ì•„ê°€ì‚¬ë‘_3kg_WA30DG2120EE.pdf")

def extract_text_from_pdf(pdf_path):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return extract_text(pdf_path)
    except Exception as e:
        print(f"PDF ì½ê¸° ì‹¤íŒ¨ {pdf_path}: {e}")
        return ""

def analyze_query_and_retrieve(query: str, retriever, llm):
    """ì¿¼ë¦¬ ë¶„ì„ + ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
    prompt = ChatPromptTemplate.from_messages([
        ('system', """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ë‹¤ìŒ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
        {{
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
            "main_topic": "ì£¼ì œ",
            "conditions": ["ì¡°ê±´1"],
            "details": ["ì„¸ë¶€ì‚¬í•­1"]
        }}"""),
        ('human', "ì§ˆë¬¸: {query}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    analysis_result = chain.invoke({"query": query})
    
    # JSON íŒŒì‹±
    try:
        data = json.loads(analysis_result)
        keywords = data.get("keywords", [])
    except Exception:
        keywords = [query]
    
    # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
    all_contexts = []
    for keyword in keywords:
        try:
            docs = retriever.invoke(keyword)
            all_contexts.extend(docs)
        except:
            continue
    
    # ì¤‘ë³µ ì œê±°
    unique = []
    seen = set()
    for doc in all_contexts:
        if doc.page_content not in seen:
            unique.append(doc)
            seen.add(doc.page_content)
    
    combined_context = "\n\n".join([doc.page_content for doc in unique[:10]])
    return combined_context, analysis_result

def enhanced_chain(query: str, retriever, llm, cot_prompt):
    context, analysis = analyze_query_and_retrieve(query, retriever, llm)
    prompt_filled = cot_prompt.invoke({
        "query": query,
        "analysis": analysis,
        "context": context
    })
    return llm.invoke(prompt_filled)

def run_chatbot():
    # âœ… ë²¡í„°í™” ì¤€ë¹„
    text = extract_text_from_pdf(FILE_PATH)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=['\n\n', '\n', ' ', '']
    )
    chunks = splitter.split_text(text)
    docs = [Document(page_content=chunk) for chunk in chunks]

    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')
    vector_store = Chroma.from_documents(
        documents=docs,
        embedding=embeddings
    )
    retriever = vector_store.as_retriever(search_type="mmr", search_kwargs={"k": 8, "fetch_k": 20})
    
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0.3)

    cot_prompt = ChatPromptTemplate.from_messages([
        ('system', """
        Elaborate on the topic using a Tree of Thoughts and backtrack when necessary to construct a clear, cohesive Chain of Thought reasoning.
        ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í•œ ê°€ì „ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
        ## ì§ˆë¬¸ ë¶„ì„
        [ë¶„ì„ ë‚´ìš©]
        ## ê´€ë ¨ ì •ë³´
        [ì •ë³´]
        ## ë‹µë³€
        ### 1. [ì¡°ê±´ A]
        ### 2. [ì¡°ê±´ B]
        ## ì¶”ê°€ ì•ˆë‚´
        """),
        ('human', """
        ì§ˆë¬¸: {query}
        ë¶„ì„: {analysis}
        ì»¨í…ìŠ¤íŠ¸: {context}
        """)
    ])

    print("="*60)
    print("ğŸ¤– ì‚¼ì„± BESPOKE AI ì½¤ë³´ ë„ìš°ë¯¸ (Mac ëŒ€ì‘ ë²„ì „)")
    print("="*60)

    while True:
        try:
            query = input("\nğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ'): ").strip()
            if query.lower() == 'ì¢…ë£Œ':
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not query:
                print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            print("ğŸ” ë‹µë³€ ìƒì„± ì¤‘...")
            result = enhanced_chain(query, retriever, llm, cot_prompt)

            print("="*60)
            print(result.content)
            print("="*60)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    run_chatbot()
